{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Loading and Handeling**"
      ],
      "metadata": {
        "id": "uuyX8jH8m6_-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnjjefD2floY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv('/content/twitter_training.csv')"
      ],
      "metadata": {
        "id": "pwTMu4-Dfx97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "Ek8HSqxLfzQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape:\", data.shape)\n",
        "print(\"Columns:\", data.columns.tolist())"
      ],
      "metadata": {
        "id": "z4stz-5tf0E0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/twitter_training.csv\", header=0, names=[\"TweetID\", \"location\", \"Sentiment\", \"Tweet content\"])\n",
        "print(\"Shape:\", data.shape)\n",
        "print(\"Columns:\", data.columns.tolist())\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "Rq5YF3dhf61o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[[\"location\", \"Sentiment\",\"Tweet content\"]]\n",
        "data = data[data[\"Sentiment\"].isin([\"Positive\", \"Negative\", \"Neutral\"])]"
      ],
      "metadata": {
        "id": "KAvOGPU-f8e8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "IwWkFLf6f-Xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "aI13Mk8of_zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.dropna(subset=['Tweet content'])\n",
        "\n"
      ],
      "metadata": {
        "id": "aSCuxFqxgBcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.duplicated().sum()"
      ],
      "metadata": {
        "id": "6X79f8yBgDDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "obFGCXepgEe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "n3igqpXsgF4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "data['Sentiment'] = data['Sentiment'].str.lower().str.strip()\n",
        "\n",
        "le = LabelEncoder()\n",
        "data['label'] = le.fit_transform(data['Sentiment'])\n",
        "\n",
        "print(\"Label mapping:\", dict(zip(le.classes_, le.transform(le.classes_))))"
      ],
      "metadata": {
        "id": "2xGKPTP0gHqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'#', '', text)\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "data['clean_text'] = data['Tweet content'].apply(clean_text)"
      ],
      "metadata": {
        "id": "bwxw44smgJZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "NrwgHbvugLCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup , Installations & Imports**"
      ],
      "metadata": {
        "id": "hHFBvEC4hhVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "oHqx4wCri2H7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T8M-tlnmgMqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip uninstall -y transformers\n"
      ],
      "metadata": {
        "id": "AxP9bAeClY26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf /usr/local/lib/python3.11/dist-packages/transformers*\n"
      ],
      "metadata": {
        "id": "nSlyylJwlYsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pip\n",
        "!pip install -U transformers==4.46.3 datasets evaluate scikit-learn\n"
      ],
      "metadata": {
        "id": "LtszQwO9m92Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(\"Transformers version:\", transformers.__version__)\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "print(\"Imports successful!\")\n"
      ],
      "metadata": {
        "id": "JaZZV_BDnxv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n"
      ],
      "metadata": {
        "id": "DHOAv0vXnyJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, temp_df = train_test_split(data, test_size=0.2, random_state=42, stratify=data[\"label\"])\n",
        "\n",
        "\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df[\"label\"])\n",
        "\n",
        "print(f\"Train size: {len(train_df)}\")\n",
        "print(f\"Validation size: {len(val_df)}\")\n",
        "print(f\"Test size: {len(test_df)}\")\n",
        "\n",
        "# Convert pandas : Hugging Face Dataset\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "train_dataset = train_dataset.remove_columns(\"__index_level_0__\")\n",
        "val_dataset = val_dataset.remove_columns(\"__index_level_0__\")\n",
        "test_dataset = test_dataset.remove_columns(\"__index_level_0__\")"
      ],
      "metadata": {
        "id": "pivQehlhoOgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Load pretrained tokenizer\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"clean_text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "# Apply tokenization to all splits\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Load BERT model for classification (3 sentiment labels)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n"
      ],
      "metadata": {
        "id": "UAO7D-Hko5sO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train and Evaluate**"
      ],
      "metadata": {
        "id": "hsSA2GBr2YU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    acc = accuracy_metric.compute(predictions=predictions, references=labels)\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
        "    return {**acc, **f1}\n",
        "\n",
        "# Smaller dataset subsets for faster training\n",
        "train_dataset_small = train_dataset.select(range(40000))\n",
        "val_dataset_small = val_dataset.select(range(2000))\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    fp16=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset_small,\n",
        "    eval_dataset=val_dataset_small,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "Zoq9OOSaAHN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test**"
      ],
      "metadata": {
        "id": "4K73huTEnIdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = trainer.evaluate(test_dataset)\n",
        "print(metrics)\n"
      ],
      "metadata": {
        "id": "vTu19jXmHRGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SAVING**"
      ],
      "metadata": {
        "id": "ANQlwUPonNyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./sentiment_model\")\n",
        "tokenizer.save_pretrained(\"./sentiment_model\")\n"
      ],
      "metadata": {
        "id": "4xRYxOwRH1p1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r sentiment_model.zip sentiment_model\n"
      ],
      "metadata": {
        "id": "AAm3NN2zKC_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"sentiment_model.zip\")\n"
      ],
      "metadata": {
        "id": "FrJVcFNoKBaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"./sentiment_model\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./sentiment_model\")\n"
      ],
      "metadata": {
        "id": "B6sj2SsBIJex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **viaualizing the predicted output**\n"
      ],
      "metadata": {
        "id": "MOQi_-NonUFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "    outputs = model(**inputs)\n",
        "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "    pred_label = torch.argmax(probs, dim=-1).item()\n",
        "\n",
        "    label_map = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
        "    return label_map[pred_label], probs[0][pred_label].item()\n",
        "\n",
        "# Example:\n",
        "text = \"I love this product!\"\n",
        "label, confidence = predict_sentiment(text)\n",
        "print(f\"Predicted sentiment: {label} with confidence {confidence:.2f}\")\n"
      ],
      "metadata": {
        "id": "HL4QnYphIQpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_sentiment(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "    outputs = model(**inputs)\n",
        "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].detach().numpy()\n",
        "\n",
        "    labels = [\"negative\", \"neutral\", \"positive\"]\n",
        "\n",
        "    plt.figure(figsize=(6,4))\n",
        "    bars = plt.bar(labels, probs, color=['red', 'gray', 'green'])\n",
        "    plt.ylim(0,1)\n",
        "    plt.title(f\"Sentiment probabilities for: '{text}'\")\n",
        "\n",
        "    for bar, prob in zip(bars, probs):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() - 0.1, f\"{prob:.2f}\", ha='center', color='white', fontsize=12)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Example:\n",
        "visualize_sentiment(\"I like this product!\")\n"
      ],
      "metadata": {
        "id": "iEZqG8BtIxcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_sentiment_pie(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "    outputs = model(**inputs)\n",
        "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].detach().numpy()\n",
        "\n",
        "    labels = [\"negative\", \"neutral\", \"positive\"]\n",
        "    colors = ['red', 'gray', 'green']\n",
        "\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.pie(probs, labels=labels, autopct='%1.1f%%', colors=colors, startangle=140)\n",
        "    plt.title(f\"Sentiment distribution for: '{text}'\")\n",
        "    plt.show()\n",
        "\n",
        "# Example:\n",
        "visualize_sentiment_pie(\"This product is perfect, but not great for me.\")\n"
      ],
      "metadata": {
        "id": "5uwXDymcI9T0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_and_visualize(text):\n",
        "    label, confidence = predict_sentiment(text)\n",
        "    print(f\"Predicted sentiment: {label} with confidence {confidence:.2f}\")\n",
        "    visualize_sentiment(text)\n",
        "\n",
        "# Example:\n",
        "predict_and_visualize(\"I'm really unhappy with this service.\")\n"
      ],
      "metadata": {
        "id": "TXkocaOWJFIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lCOI27hPIJIS"
      }
    }
  ]
}